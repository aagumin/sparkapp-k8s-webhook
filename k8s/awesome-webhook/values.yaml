# Default values for awesome-webhook.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: awesome-webhook
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: 0.1.0

imagePullSecrets: [ ]
nameOverride: ""
fullnameOverride: ""
podAnnotations: { }
podSecurityContext: { }
# fsGroup: 2000

securityContext:
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 10001
  privileged: false
  allowPrivilegeEscalation: false

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

service:
  type: ClusterIP
  port: "9443"
  protocol: https

readinessProbePath: /health
livenessProbePath: /health

resources:
  limits:
    cpu: "2"
    memory: 4Gi
  requests:
    cpu: "2"
    memory: 2Gi


nodeSelector: { }

tolerations: [ ]

affinity: { }

mutate:
  mutateUrl: "/mutate"
  apiGroups:
    - "*"
  apiVersions:
    - "*"
  operations:
    - CREATE
  resources:
    - SparkApplication
    - SparkApplications
    - sparkapplications
    - sparkapplication
    - sparkapp
    - sparkapplications.sparkoperator.k8s.io
    - scheduledsparkapplications.sparkoperator.k8s.io
  scope: Namespaced
  sideEffects: None
  timeoutSeconds: 20
  objectSelector:
    matchExpressions:
      - key: deploymentApiType
        operator: NotIn
        values: ["nrt_job"]
      - key: example.webhook.status
        operator: NotIn
        values: ["disable"]


appConfig:
  spark:
    configMapName: asmwebhook-configmap
    configMapValue: |
      FeatureList:
        Toleration:
          enabled: True
          hardPatch: True
        Affinity:
          enabled: True
          hardPatch: True
        HistoryServer:
          enabled: True
          hardPatch: True
        PrometheusMonitoring:
          enabled: True
          hardPatch: True
  
      SparkPatchValue:
        AmazmeSparkToleration:
          ml:
            - key: "app"
              operator: "Equal"
              value: "spark"
              effect: "NoExecute"
      
        AmazmeSparkAffinity:
          ml:
            nodeaffinity:
              requiredduringschedulingignoredduringexecution:
                nodeselectorterms:
                  - matchexpressions:
                      - key: app.kubernetes.io/component
                        operator: In
                        values:
                          - spark
      
        AmazmeSparkHistoryServerSpark:
          spark.eventLog.enabled: "true"
          spark.eventLog.dir: "s3a://prod/spark-events"
          spark.eventLog.rolling.enabled: "true"
          spark.eventLog.rolling.maxFileSize: "128m"
      
        AmazmeSparkHistoryServerHadoop:
          fs.s3a.aws.credentials.provider: "com.amazonaws.auth.EnvironmentVariableCredentialsProvider"
          fs.s3a.endpoint: "https://s3.magic.ru"
          fs.s3a.path.style.access: "true"
          fs.s3a.connection.ssl.enabled: "false" # TODO: remove this when we will fix it

containerEnv:
  - name: LOG_LEVEL
    value: INFO
  - name: SPARK_CFG_PATH
    value: /etc/configs/spark.yaml
